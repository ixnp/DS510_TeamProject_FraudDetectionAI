{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbe8bf2",
      "metadata": {
        "id": "0bbe8bf2"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # Credit Card Fraud Detection\n",
        "#\n",
        "# This script is formatted as a Jupyter-friendly Python script (use with JupyterLab/Notebook or run as a .py with an editor that understands `# %%` cells).\n",
        "#\n",
        "# **Dataset**: `creditcard.csv` (Kaggle: mlg-ulb/creditcardfraud)\n",
        "# Place `creditcard.csv` in the same folder as this notebook or update the path in the data loading cell.\n",
        "#\n",
        "# Sections:\n",
        "# 1. Setup & installs\n",
        "# 2. Load data & quick checks\n",
        "# 3. Exploratory Data Analysis (EDA)\n",
        "# 4. Preprocessing & feature engineering\n",
        "# 5. Train/Test split + balancing (SMOTE example)\n",
        "# 6. Modeling (LogisticRegression, RandomForest, XGBoost)\n",
        "# 7. Evaluation & metrics\n",
        "# 8. Save model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b57d85",
      "metadata": {},
      "source": [
        "## 1) Setup - installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd39f75a",
      "metadata": {
        "id": "bd39f75a"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q imbalanced-learn xgboost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57291de4",
      "metadata": {},
      "source": [
        "## 2) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba389338",
      "metadata": {
        "id": "ba389338"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_recall_curve, roc_curve, auc\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Try/except for XGBoost since it may not be installed by default\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "except Exception:\n",
        "    xgb_available = False\n",
        "\n",
        "# For handling imbalance\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f895465a",
      "metadata": {
        "id": "f895465a"
      },
      "source": [
        "## 3) Load data & quick checks\n",
        "### Ensure `creditcard.csv` is present in the same directory. If you downloaded from Kaggle, the filename should match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a31c0df",
      "metadata": {
        "id": "2a31c0df"
      },
      "outputs": [],
      "source": [
        "# Adjust path if needed\n",
        "DATA_PATH = '/kaggle/input/creditcardfraud/creditcard.csv'\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"{DATA_PATH} not found. Put creditcard.csv in the working directory or change DATA_PATH.\")\n",
        "\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print('Shape:', data.shape)\n",
        "print('\\nColumns:', data.columns.tolist())\n",
        "\n",
        "# Quick peek\n",
        "display(data.head())\n",
        "\n",
        "# Basic info\n",
        "print('\\nInfo:')\n",
        "print(data.info())\n",
        "\n",
        "print('\\nMissing values per column:')\n",
        "print(data.isnull().sum().sort_values(ascending=False).head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7016f710",
      "metadata": {
        "id": "7016f710"
      },
      "source": [
        "### Target distribution (class imbalance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac550443",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "ac550443",
        "outputId": "f62c4415-0eba-4b64-c5bb-7794b04804a2"
      },
      "outputs": [],
      "source": [
        "print(data['Class'].value_counts())\n",
        "print('\\nProportions:')\n",
        "print(data['Class'].value_counts(normalize=True))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='Class', data=data)\n",
        "plt.title('Class distribution (0 = non-fraud, 1 = fraud)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8798812",
      "metadata": {
        "id": "d8798812"
      },
      "source": [
        "## 4) Exploratory Data Analysis (EDA)\n",
        "### We'll inspect `Amount`, `Time`, and a few PCA components `V1`..`V28` for differences between fraud and non-fraud.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e61d95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "00e61d95",
        "outputId": "604cd9cf-082b-47f8-caa5-dfd78caa9991"
      },
      "outputs": [],
      "source": [
        "# Amount distribution\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "sns.histplot(data['Amount'], bins=50, kde=False)\n",
        "plt.title('Transaction Amount (raw)')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.boxplot(x='Class', y='Amount', data=data)\n",
        "plt.title('Amount by Class')\n",
        "plt.show()\n",
        "\n",
        "# Log-transformed Amount (optional, helps models)\n",
        "data['Amount_log'] = np.log1p(data['Amount'])\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(x='Class', y='Amount_log', data=data)\n",
        "plt.title('Log(Amount+1) by Class')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f436d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "44f436d8",
        "outputId": "ae07aa17-afe8-419e-e671-63526e09842f"
      },
      "outputs": [],
      "source": [
        "# Time: convert seconds to hours-of-day roughly (if you want to look for time-of-day patterns).\n",
        "# Note: 'Time' is seconds elapsed between each transaction and the first transaction in the dataset.\n",
        "# We will look at simple distribution splits.\n",
        "\n",
        "data['Hour'] = (data['Time'] // 3600) % 24\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.histplot(data[data['Class']==0]['Hour'], label='Non-fraud', stat='count', bins=24, alpha=0.6)\n",
        "sns.histplot(data[data['Class']==1]['Hour'], label='Fraud', stat='count', bins=24, alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title('Transaction Counts by Hour (approx)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7dce88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "4e7dce88",
        "outputId": "5f2ca9e8-a547-4fb1-b9c9-7319483d30db"
      },
      "outputs": [],
      "source": [
        "# Compare a few PCA components distributions between classes\n",
        "pca_cols = [c for c in data.columns if c.startswith('V')][:6]  # show first 6 for brevity\n",
        "plt.figure(figsize=(14,8))\n",
        "for i, col in enumerate(pca_cols, 1):\n",
        "    plt.subplot(3,2,i)\n",
        "    sns.kdeplot(data.loc[data['Class']==0, col], label='Non-fraud', common_norm=False)\n",
        "    sns.kdeplot(data.loc[data['Class']==1, col], label='Fraud', common_norm=False)\n",
        "    plt.title(col)\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf4749d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf4749d",
        "outputId": "f86556ef-d54d-4891-c9d1-2e831e26c481"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix — we will look at correlation with Class only to keep it readable\n",
        "corr_with_class = data.corr()['Class'].sort_values(ascending=False)\n",
        "print('Top correlations with Class:')\n",
        "print(corr_with_class.head(10))\n",
        "print('\\nLowest correlations with Class:')\n",
        "print(corr_with_class.tail(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7c4635",
      "metadata": {
        "id": "af7c4635"
      },
      "source": [
        "## 5) Preprocessing & feature engineering\n",
        "- Drop columns we don't need (raw `Amount` maybe kept but we will prefer `Amount_log`)\n",
        "- Scale features (StandardScaler) where appropriate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a633e155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a633e155",
        "outputId": "738deb84-adda-40b0-cbb0-015e702aa576"
      },
      "outputs": [],
      "source": [
        "FEATURES = [c for c in data.columns if c not in ['Class', 'Time']]  # keep Time-derived features if needed\n",
        "TARGET = 'Class'\n",
        "\n",
        "# We'll drop 'Amount' and 'Time' from direct modeling but keep Amount_log and Hour (optional)\n",
        "if 'Amount' in FEATURES:\n",
        "    FEATURES.remove('Amount')\n",
        "if 'Time' in FEATURES:\n",
        "    pass\n",
        "\n",
        "X = data[FEATURES]\n",
        "y = data[TARGET]\n",
        "\n",
        "# Standardize 'Amount_log' and optionally 'Hour' (hours are cyclic but this is a simple approach)\n",
        "scaler = StandardScaler()\n",
        "X[['Amount_log', 'Hour']] = scaler.fit_transform(X[['Amount_log', 'Hour']])\n",
        "\n",
        "print('Feature matrix shape:', X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba6f8d31",
      "metadata": {
        "id": "ba6f8d31"
      },
      "source": [
        "## 6) Train/Test split and handling class imbalance\n",
        "- We'll use a stratified split to preserve class ratios. Then apply SMOTE on training set to balance classes for some models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb94b5ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb94b5ee",
        "outputId": "ddf40189-9e3a-4331-9be0-6fcd2081c17e"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print('Train class distribution:')\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print('\\nTest class distribution:')\n",
        "print(y_test.value_counts(normalize=True))\n",
        "\n",
        "# Apply SMOTE on training data\n",
        "smote = SMOTE(random_state=42, sampling_strategy='auto')  # will oversample minority to match majority\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print('\\nAfter SMOTE - train distribution:')\n",
        "print(pd.Series(y_train_res).value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b3b680",
      "metadata": {
        "id": "f8b3b680"
      },
      "source": [
        "## 7) Modeling — baseline models\n",
        "### We'll train Logistic Regression, Random Forest, and XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef8f52f",
      "metadata": {
        "id": "1ef8f52f"
      },
      "outputs": [],
      "source": [
        "models = {}\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_res, y_train_res)\n",
        "models['LogisticRegression'] = lr\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "models['RandomForest'] = rf\n",
        "\n",
        "# XGBoost (optional)\n",
        "if xgb_available:\n",
        "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    xgb.fit(X_train_res, y_train_res)\n",
        "    models['XGBoost'] = xgb\n",
        "else:\n",
        "    print('XGBoost not available — skip.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5487a0d",
      "metadata": {
        "id": "d5487a0d"
      },
      "source": [
        "## 8) Evaluation helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b28551",
      "metadata": {
        "id": "e1b28551"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, X_test, y_test, name=None):\n",
        "    if name is None:\n",
        "        name = model.__class__.__name__\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = None\n",
        "    try:\n",
        "        y_proba = model.predict_proba(X_test)[:,1]\n",
        "    except Exception:\n",
        "        try:\n",
        "            y_proba = model.decision_function(X_test)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "    if y_proba is not None:\n",
        "        roc = roc_auc_score(y_test, y_proba)\n",
        "        print('ROC AUC:', round(roc,4))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print('Confusion matrix:\\n', cm)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599fbb11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599fbb11",
        "outputId": "f26150c8-bb20-4212-c5d9-ae890a26bf38"
      },
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "for name, mdl in models.items():\n",
        "    eval_model(mdl, X_test, y_test, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f549cb48",
      "metadata": {
        "id": "f549cb48"
      },
      "source": [
        "# Precision-Recall curve for the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e1959b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "37e1959b",
        "outputId": "b127e290-5d9b-4fc8-f6ec-402979ae471d"
      },
      "outputs": [],
      "source": [
        "best_model_name = 'RandomForest' if 'RandomForest' in models else list(models.keys())[0]\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "try:\n",
        "    y_scores = best_model.predict_proba(X_test)[:,1]\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall Curve ({best_model_name})')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print('Could not plot PR curve:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e075154",
      "metadata": {
        "id": "3e075154"
      },
      "source": [
        "## 9) Feature importance (for tree-based models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19491b35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19491b35",
        "outputId": "d3a347af-ac14-4089-f4b8-177e5aca308c"
      },
      "outputs": [],
      "source": [
        "if 'RandomForest' in models:\n",
        "    importances = models['RandomForest'].feature_importances_\n",
        "    fi = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "    display(fi.head(20))\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(x=fi.head(20), y=fi.head(20).index)\n",
        "    plt.title('Top 20 Feature Importances (RandomForest)')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d72f336",
      "metadata": {
        "id": "0d72f336"
      },
      "source": [
        "## 10) Save the best model to disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b893cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93b893cd",
        "outputId": "c725b55c-afb2-46c8-ddb1-ff7609b5ba44"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "MODEL_PATH = 'best_model.joblib'\n",
        "joblib.dump(best_model, MODEL_PATH)\n",
        "print('Saved best model to', MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f912bf4b",
      "metadata": {
        "id": "f912bf4b"
      },
      "source": [
        "## Notes, next steps & improvements\n",
        "- Try hyperparameter tuning (GridSearchCV or RandomizedSearchCV) with `roc_auc` as scoring.\n",
        "- Try other balancing strategies (ADASYN, undersampling, ensemble approaches).\n",
        "- Use time-based validation if transactions are time-ordered (i.e., avoid leakage by splitting by time).\n",
        "- Feature engineering ideas: rolling statistics per card-holder (if card-holder id exists), time-deltas, frequency of transactions.\n",
        "- Consider cost-sensitive learning (penalize false negatives higher than false positives).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
